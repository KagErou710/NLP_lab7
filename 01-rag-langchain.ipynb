{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Natural Language Processing\n",
    "\n",
    "# Retrieval-Augmented generation (RAG)\n",
    "\n",
    "RAG is a technique for augmenting LLM knowledge with additional, often private or real-time, data.\n",
    "\n",
    "LLMs can reason about wide-ranging topics, but their knowledge is limited to the public data up to a specific point in time that they were trained on. If you want to build AI applications that can reason about private data or data introduced after a model’s cutoff date, you need to augment the knowledge of the model with the specific information it needs.\n",
    "\n",
    "<img src=\"../figures/RAG-process.png\" >\n",
    "\n",
    "Introducing `ChakyBot`, an innovative chatbot designed to assist Chaky (the instructor) and TA (Gun) in explaining the lesson of the NLP course to students. Leveraging LangChain technology, ChakyBot excels in retrieving information from documents, ensuring a seamless and efficient learning experience for students engaging with the NLP curriculum.\n",
    "\n",
    "1. Prompt\n",
    "2. Retrieval\n",
    "3. Memory\n",
    "4. Chain"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "'pip' is not recognized as an internal or external command,\n",
      "operable program or batch file.\n",
      "'pip' is not recognized as an internal or external command,\n",
      "operable program or batch file.\n",
      "'pip' is not recognized as an internal or external command,\n",
      "operable program or batch file.\n",
      "'pip' is not recognized as an internal or external command,\n",
      "operable program or batch file.\n",
      "'pip' is not recognized as an internal or external command,\n",
      "operable program or batch file.\n",
      "'pip' is not recognized as an internal or external command,\n",
      "operable program or batch file.\n",
      "'pip' is not recognized as an internal or external command,\n",
      "operable program or batch file.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "'pip' is not recognized as an internal or external command,\n",
      "operable program or batch file.\n",
      "'pip' is not recognized as an internal or external command,\n",
      "operable program or batch file.\n"
     ]
    }
   ],
   "source": [
    "#langchain library\n",
    "!pip install langchain==0.0.350\n",
    "#LLM\n",
    "!pip install accelerate==0.25.0\n",
    "!pip install transformers==4.36.2\n",
    "!pip install bitsandbytes==0.41.2\n",
    "#Text Embedding\n",
    "!pip install sentence-transformers==2.2.2\n",
    "!pip install InstructorEmbedding==1.0.1\n",
    "#vectorstore\n",
    "!pip install pymupdf==1.23.8\n",
    "!pip install faiss-gpu==1.7.2\n",
    "!pip install faiss-cpu==1.7.4"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\Tairo Kageyama\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\tqdm\\auto.py:22: TqdmWarning:\n",
      "\n",
      "IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "device(type='cpu')"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import os\n",
    "import torch\n",
    "# Set GPU device\n",
    "# os.environ[\"CUDA_VISIBLE_DEVICES\"] = \"1\"\n",
    "\n",
    "# os.environ['http_proxy']  = 'http://192.41.170.23:3128'\n",
    "# os.environ['https_proxy'] = 'http://192.41.170.23:3128'\n",
    "\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "device"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'\\n\\n'"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\"\"\"\n",
    "\n",
    "\"\"\""
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Prompt\n",
    "\n",
    "A set of instructions or input provided by a user to guide the model's response, helping it understand the context and generate relevant and coherent language-based output, such as answering questions, completing sentences, or engaging in a conversation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "PromptTemplate(input_variables=['context', 'question'], template=\"I'm your friendly chatbot named AIT-GPT, here to assist you with any questions you have about AIT. \\n    If you are interesting about Background of AIT, Programs, Degrees, Admissions, Tuitions fee, Ongoing Research, anything about AIT, please feel free to ask.\\n    I am covering everything in official web page. So if you felt lazy to search in web page by yourself, no hesitate to asking me.\\n    {context}\\n    Question: {question}\\n    Answer:\")"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from langchain import PromptTemplate\n",
    "\n",
    "prompt_template = \"\"\"\n",
    "    I'm your friendly chatbot named AIT-GPT, here to assist you with any questions you have about AIT. \n",
    "    If you are interesting about Background of AIT, Programs, Degrees, Admissions, Tuitions fee, Ongoing Research, anything about AIT, please feel free to ask.\n",
    "    I am covering everything in official web page. So if you felt lazy to search in web page by yourself, no hesitate to asking me.\n",
    "    {context}\n",
    "    Question: {question}\n",
    "    Answer:\n",
    "    \"\"\".strip()\n",
    "\n",
    "    # I'm your friendly NLP chatbot named ChakyBot, here to assist Chaky and Gun with any questions they have about Natural Language Processing (NLP). \n",
    "    # If you're curious about how probability works in the context of NLP, feel free to ask any questions you may have. \n",
    "    # Whether it's about probabilistic models, language models, or any other related topic, \n",
    "    # I'm here to help break down complex concepts into easy-to-understand explanations.\n",
    "    # Just let me know what you're wondering about, and I'll do my best to guide you through it!\n",
    "\n",
    "PROMPT = PromptTemplate.from_template(\n",
    "    template = prompt_template\n",
    ")\n",
    "\n",
    "PROMPT\n",
    "#using str.format \n",
    "#The placeholder is defined using curly brackets: {} {}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\"I'm your friendly chatbot named AIT-GPT, here to assist you with any questions you have about AIT. \\n    If you are interesting about Background of AIT, Programs, Degrees, Admissions, Tuitions fee, Ongoing Research, anything about AIT, please feel free to ask.\\n    I am covering everything in official web page. So if you felt lazy to search in web page by yourself, no hesitate to asking me.\\n    Machine learning (ML) is a field of study in artificial intelligence concerned with the development and study of statistical algorithms that can effectively generalize and thus perform tasks without explicit instructions.\\n    Question: What is Machine Learning\\n    Answer:\""
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "PROMPT.format(\n",
    "    context = \"I heard you want to go to AIT\",\n",
    "    question = \"What is AIT\"\n",
    ")"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Note : [How to improve prompting (Zero-shot, Few-shot, Chain-of-Thought, etc.](https://github.com/chaklam-silpasuwanchai/Natural-Language-Processing/blob/main/Code/05%20-%20RAG/advance/cot-tot-prompting.ipynb)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Retrieval\n",
    "\n",
    "1. `Document loaders` : Load documents from many different sources (HTML, PDF, code). \n",
    "2. `Document transformers` : One of the essential steps in document retrieval is breaking down a large document into smaller, relevant chunks to enhance the retrieval process.\n",
    "3. `Text embedding models` : Embeddings capture the semantic meaning of the text, allowing you to quickly and efficiently find other pieces of text that are similar.\n",
    "4. `Vector stores`: there has emerged a need for databases to support efficient storage and searching of these embeddings.\n",
    "5. `Retrievers` : Once the data is in the database, you still need to retrieve it."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.1 Document Loaders \n",
    "Use document loaders to load data from a source as Document's. A Document is a piece of text and associated metadata. For example, there are document loaders for loading a simple .txt file, for loading the text contents of any web page, or even for loading a transcript of a YouTube video.\n",
    "\n",
    "[PDF Loader](https://python.langchain.com/docs/modules/data_connection/document_loaders/pdf)\n",
    "\n",
    "[Download Document](https://web.stanford.edu/~jurafsky/slp3/)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_community.document_loaders import PyMuPDFLoader\n",
    "\n",
    "nlp_docs = r'C:\\Users\\Tairo Kageyama\\Documents\\GitHub\\Python-fo-Natural-Language-Processing-main\\lab7\\Data\\AIT_web_reduced.pdf'\n",
    "\n",
    "loader = PyMuPDFLoader(nlp_docs)\n",
    "documents = loader.load()\n",
    "\n",
    "# import PyPDF2\n",
    "# from spacy import displacy\n",
    "\n",
    "# reader = PyPDF2.PdfReader(\"Data/Resume.pdf\")\n",
    "# page = reader.pages[0]\n",
    "# documents = page.extract_text()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[Document(page_content='Home > Data Science and AI (DSAI)\\nData Science and AI (DSAI)\\nData Science (DS) is concerned with the extraction of useful knowledge from data sets.\\xa0 It is closely\\nrelated to the ﬁelds of computer science, mathematics, and statistics.\\xa0 It is a relatively new term for a\\nbroad set of skills spanning the more established ﬁelds of machine learning, data mining, databases, and\\nvisualization, along with their applications in various ﬁelds.\\xa0 In 2012, Harvard Business Review called\\ndata science “The Sexiest Job of the 21\\n Century”.\\nAr�ﬁcial Intelligence (AI) is the broad ﬁeld conceived in 1956 as the automation or simulation of human\\nintelligence.\\xa0 AI has two primary “levels”.\\xa0 The ﬁrst level, “narrow AI”, concerns perception, statistical\\ninference, and actuation, drawing on data science, sensors, and robotics.\\xa0 The second level, sometimes\\ncalled “artiﬁcial general intelligence (AGI), is concerned with more complex or ﬂexible reasoning and\\ndecision-making in less constrained domains.\\nThe AIT Masters in DS&AI was designed in partnership with the Erasmus+ DS&AI consortium, a group\\nof 15 European and Asian universities with the mission of bringing European-standard advanced\\neducation to Asia.\\nst\\nWe use cookies on our website to give you the most relevant experience by remembering your preferences and\\nrepeat visits. By clicking “Accept All”, you consent to the use of ALL the cookies. However, you may visit \"Cookie\\nSettings\" to provide a controlled consent.\\nCookie Settings\\nAccept All\\n', metadata={'source': 'C:\\\\Users\\\\Tairo Kageyama\\\\Documents\\\\GitHub\\\\Python-fo-Natural-Language-Processing-main\\\\lab7\\\\Data\\\\AIT_web_reduced.pdf', 'file_path': 'C:\\\\Users\\\\Tairo Kageyama\\\\Documents\\\\GitHub\\\\Python-fo-Natural-Language-Processing-main\\\\lab7\\\\Data\\\\AIT_web_reduced.pdf', 'page': 0, 'total_pages': 8, 'format': 'PDF 1.4', 'title': '', 'author': '', 'subject': '', 'keywords': '', 'creator': '', 'producer': 'iLovePDF', 'creationDate': '', 'modDate': 'D:20240319123827Z', 'trapped': ''}),\n",
       " Document(page_content='Research Focus Area\\nPreferred background\\nTo study in the DS&AI ﬁeld, the students should fulﬁll one of the following backgrounds.\\nCourse Structure\\nData modeling and management\\nMachine learning\\nData mining\\nData science\\nSensors\\nRobotics\\nSoftware development\\nArtiﬁcial intelligence\\nBig Data and Deep Learning\\nComputer Science/Computer Engineering/ICT.\\nEngineering background with work experience, mathematical skills, and programming skills.\\nDiverse backgrounds such as business, ﬁnance, or other non-engineering ﬁelds. Candidates must\\ntake a foundation course in calculus, discrete mathematics, linear algebra, and basic computer\\nprogramming.\\nProgram Brochure\\nDownload\\n', metadata={'source': 'C:\\\\Users\\\\Tairo Kageyama\\\\Documents\\\\GitHub\\\\Python-fo-Natural-Language-Processing-main\\\\lab7\\\\Data\\\\AIT_web_reduced.pdf', 'file_path': 'C:\\\\Users\\\\Tairo Kageyama\\\\Documents\\\\GitHub\\\\Python-fo-Natural-Language-Processing-main\\\\lab7\\\\Data\\\\AIT_web_reduced.pdf', 'page': 1, 'total_pages': 8, 'format': 'PDF 1.4', 'title': '', 'author': '', 'subject': '', 'keywords': '', 'creator': '', 'producer': 'iLovePDF', 'creationDate': '', 'modDate': 'D:20240319123827Z', 'trapped': ''}),\n",
       " Document(page_content='Major in DS&AI\\nRequired courses\\nElective courses\\nMaster of Science/Master of Engineering in Data Science and Ar�ﬁcial Intelligence.\\nThesis Option\\nResearch Study Option\\nRequired Courses\\n14 credits (5 courses)\\n14 credits (5 courses)\\nElec�ve Courses\\n6 credits (2 courses)\\n18 credits (6 courses)\\nIns�tute Wide Courses\\n3 credits (1 course)\\n3 credits (1 course)\\nSeminar: Required Pass/Fail\\n1 credit\\n1 credit\\nRequired Internship\\n0 credit\\n0 credit\\nTotal Credits Coursework\\n24 credit\\n36 credit\\nThesis/Research Study Credits\\n24 credit\\n12 credit\\nTOTAL CREDIT REQUIREMENT\\n48 credit\\n48 credit\\nData Modeling and Management\\nMachine Learning\\nBusiness Intelligence and Analytics\\nComputer Programming for Data Science and Artiﬁcial Intelligence\\nArtiﬁcial Intelligence: Natural Language Understanding\\nArtiﬁcial Intelligence: Knowledge Representation and Reasoning\\nComputer Vision\\nArtiﬁcial Intelligence: Problem Solving and Planning\\n', metadata={'source': 'C:\\\\Users\\\\Tairo Kageyama\\\\Documents\\\\GitHub\\\\Python-fo-Natural-Language-Processing-main\\\\lab7\\\\Data\\\\AIT_web_reduced.pdf', 'file_path': 'C:\\\\Users\\\\Tairo Kageyama\\\\Documents\\\\GitHub\\\\Python-fo-Natural-Language-Processing-main\\\\lab7\\\\Data\\\\AIT_web_reduced.pdf', 'page': 2, 'total_pages': 8, 'format': 'PDF 1.4', 'title': '', 'author': '', 'subject': '', 'keywords': '', 'creator': '', 'producer': 'iLovePDF', 'creationDate': '', 'modDate': 'D:20240319123827Z', 'trapped': ''}),\n",
       " Document(page_content='HCI and Information Visualization\\nRecent Trends in Machine Learning\\nMulticriteria Optimization and Decision Analysis\\nSoftware Development and Project Management\\nMinor in DS&AI\\nMaster of Science/Master of Engineering, Minor in Data Science and Ar�ﬁcial Intelligence.\\nResearch\\nStudy\\nOption\\nMajor\\nRequired/Elec�ve\\nCourses\\n24 credits\\nRequired Courses\\nin DS&AI\\n6 credits\\n(2 courses)\\n· Data Modeling and Management · Machine Learning\\nElec�ve Courses in\\nDS&AI\\n6 credits\\n(2 courses)\\nChoose 2 courses from 8 op�ons listed: · Business\\nIntelligence and Analy�cs · Computer Programming for\\nData Science and Ar�ﬁcial Intelligence · Ar�ﬁcial\\nIntelligence: Problem Solving and Planning · Ar�ﬁcial\\nIntelligence: Knowledge Representa�on and Reasoning\\n· Computer Vision · Ar�ﬁcial Intelligence: Natural\\nLanguage Understanding · Recent Trend in Machine\\nLearning · Mul�criteria Op�miza�on and Decision\\nAnalysis\\nTotal Credits\\nCoursework\\n36 credits\\nResearch Study\\nCredits\\n12 credits\\nTOTAL CREDIT\\nREQUIREMENT\\n48 credits\\n', metadata={'source': 'C:\\\\Users\\\\Tairo Kageyama\\\\Documents\\\\GitHub\\\\Python-fo-Natural-Language-Processing-main\\\\lab7\\\\Data\\\\AIT_web_reduced.pdf', 'file_path': 'C:\\\\Users\\\\Tairo Kageyama\\\\Documents\\\\GitHub\\\\Python-fo-Natural-Language-Processing-main\\\\lab7\\\\Data\\\\AIT_web_reduced.pdf', 'page': 3, 'total_pages': 8, 'format': 'PDF 1.4', 'title': '', 'author': '', 'subject': '', 'keywords': '', 'creator': '', 'producer': 'iLovePDF', 'creationDate': '', 'modDate': 'D:20240319123827Z', 'trapped': ''}),\n",
       " Document(page_content='Research\\nStudy\\nOption\\nConcerned Faculty\\nStaff\\nChu�porn\\nAnutariya\\nAssociate Professor\\nChaklam\\nSilpasuwanchai\\nAssistant Professor\\nVatcharaporn\\nEsichaikul\\nAssociate Professor\\n', metadata={'source': 'C:\\\\Users\\\\Tairo Kageyama\\\\Documents\\\\GitHub\\\\Python-fo-Natural-Language-Processing-main\\\\lab7\\\\Data\\\\AIT_web_reduced.pdf', 'file_path': 'C:\\\\Users\\\\Tairo Kageyama\\\\Documents\\\\GitHub\\\\Python-fo-Natural-Language-Processing-main\\\\lab7\\\\Data\\\\AIT_web_reduced.pdf', 'page': 4, 'total_pages': 8, 'format': 'PDF 1.4', 'title': '', 'author': '', 'subject': '', 'keywords': '', 'creator': '', 'producer': 'iLovePDF', 'creationDate': '', 'modDate': 'D:20240319123827Z', 'trapped': ''}),\n",
       " Document(page_content='Apply Now\\nResearch Focus Area\\nPreferred background\\nCourse Structure\\nConcerned Faculty\\nStaff\\nContact Us\\nContact Us\\nComputer Science & Information Management (CSIM)\\nDepartment of ICT\\xa0\\nSchool of Engineering and Technology (SET)\\xa0\\nAsian Institute of Technology (AIT)\\xa0\\nPhone:+66(0)25245700; +66(0)25245717\\xa0\\nEmail:dsai@ait.ac.th\\nSiriporn Nanthasing\\nSireekant\\nThanwongpan\\n', metadata={'source': 'C:\\\\Users\\\\Tairo Kageyama\\\\Documents\\\\GitHub\\\\Python-fo-Natural-Language-Processing-main\\\\lab7\\\\Data\\\\AIT_web_reduced.pdf', 'file_path': 'C:\\\\Users\\\\Tairo Kageyama\\\\Documents\\\\GitHub\\\\Python-fo-Natural-Language-Processing-main\\\\lab7\\\\Data\\\\AIT_web_reduced.pdf', 'page': 5, 'total_pages': 8, 'format': 'PDF 1.4', 'title': '', 'author': '', 'subject': '', 'keywords': '', 'creator': '', 'producer': 'iLovePDF', 'creationDate': '', 'modDate': 'D:20240319123827Z', 'trapped': ''}),\n",
       " Document(page_content='For more informa�on\\n+66(0)25245700; +66(0)25245717 \\ndsai@ait.ac.th\\nP.O. Box 4, 58 Moo 9, Km. 42, Paholyothin Highway, Klong Luang, Pathum Thani 12120 Thailand\\n(+66) 25245000, (+66) 25160110-44\\nAbout\\nAbout AIT\\nFacts and ﬁgures\\nRankings\\nLeadership\\nPeople\\nMeet our faculty\\nMeet our staff\\nAcademics\\nAcademic calendar\\nAcademic Programs\\nStudy options\\nStudent opportunities\\nSchools\\nApply to AIT\\nAdmissions\\nFinancial aid\\nTuition and fees\\nStudent housing\\nApply online\\n', metadata={'source': 'C:\\\\Users\\\\Tairo Kageyama\\\\Documents\\\\GitHub\\\\Python-fo-Natural-Language-Processing-main\\\\lab7\\\\Data\\\\AIT_web_reduced.pdf', 'file_path': 'C:\\\\Users\\\\Tairo Kageyama\\\\Documents\\\\GitHub\\\\Python-fo-Natural-Language-Processing-main\\\\lab7\\\\Data\\\\AIT_web_reduced.pdf', 'page': 6, 'total_pages': 8, 'format': 'PDF 1.4', 'title': '', 'author': '', 'subject': '', 'keywords': '', 'creator': '', 'producer': 'iLovePDF', 'creationDate': '', 'modDate': 'D:20240319123827Z', 'trapped': ''}),\n",
       " Document(page_content='Home > Research Overview\\nResearch Overview\\nRecognized as one of the leading academic institutes in the ﬁeld of sustainability in Asia, AIT through its\\nresearch projects, thrives to take-on new research frontiers to build a better knowledge base and\\nfacilitate actions towards the sustainable development of the region. With an aim to make a signiﬁcant\\nand lasting difference to the communities, AIT’s research projects are designed to bolster technology\\ndevelopment and application, environmental conservation, policy innovation thus promoting\\nsustainability and sustainable development around Asia and beyond.\\nAIT equipped with over six-decade-long research and academic experience has effectively built and\\nstrengthened regional and international partnerships, bridged interdisciplinary teams through its\\npartnerships to address regional and global challenges\\xa0AIT research is importantly aligned to all 17\\nSDG’s, contributing to the sustainable development of the region, strengthening the knowledge\\ndevelopment and business capacity, and supporting communities with their economic development and\\nintegration into the global economy. AIT focuses on assisting stakeholders build their capacity to\\npromote sustainability through appropriate technology, relevant and applied research, sustainable\\nframeworks for development and planning, informed policy making and practice applications in the\\nregion.\\xa0AIT’s ﬁve thematic areas of research are, namely, Climate Change; Smart Communities;\\nInfrastructure; Technology, Policy, Society and Water-Energy-Food.\\xa0\\nIn this sec�on\\nWe use cookies on our website to give you the most relevant experience by remembering your preferences and\\nrepeat visits. By clicking “Accept All”, you consent to the use of ALL the cookies. However, you may visit \"Cookie\\nSettings\" to provide a controlled consent.\\nCookie Settings\\nAccept All\\n', metadata={'source': 'C:\\\\Users\\\\Tairo Kageyama\\\\Documents\\\\GitHub\\\\Python-fo-Natural-Language-Processing-main\\\\lab7\\\\Data\\\\AIT_web_reduced.pdf', 'file_path': 'C:\\\\Users\\\\Tairo Kageyama\\\\Documents\\\\GitHub\\\\Python-fo-Natural-Language-Processing-main\\\\lab7\\\\Data\\\\AIT_web_reduced.pdf', 'page': 7, 'total_pages': 8, 'format': 'PDF 1.4', 'title': '', 'author': '', 'subject': '', 'keywords': '', 'creator': '', 'producer': 'iLovePDF', 'creationDate': '', 'modDate': 'D:20240319123827Z', 'trapped': ''})]"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "documents"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "8"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(documents)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Document(page_content='Home > Data Science and AI (DSAI)\\nData Science and AI (DSAI)\\nData Science (DS) is concerned with the extraction of useful knowledge from data sets.\\xa0 It is closely\\nrelated to the ﬁelds of computer science, mathematics, and statistics.\\xa0 It is a relatively new term for a\\nbroad set of skills spanning the more established ﬁelds of machine learning, data mining, databases, and\\nvisualization, along with their applications in various ﬁelds.\\xa0 In 2012, Harvard Business Review called\\ndata science “The Sexiest Job of the 21\\n Century”.\\nAr�ﬁcial Intelligence (AI) is the broad ﬁeld conceived in 1956 as the automation or simulation of human\\nintelligence.\\xa0 AI has two primary “levels”.\\xa0 The ﬁrst level, “narrow AI”, concerns perception, statistical\\ninference, and actuation, drawing on data science, sensors, and robotics.\\xa0 The second level, sometimes\\ncalled “artiﬁcial general intelligence (AGI), is concerned with more complex or ﬂexible reasoning and\\ndecision-making in less constrained domains.\\nThe AIT Masters in DS&AI was designed in partnership with the Erasmus+ DS&AI consortium, a group\\nof 15 European and Asian universities with the mission of bringing European-standard advanced\\neducation to Asia.\\nst\\nWe use cookies on our website to give you the most relevant experience by remembering your preferences and\\nrepeat visits. By clicking “Accept All”, you consent to the use of ALL the cookies. However, you may visit \"Cookie\\nSettings\" to provide a controlled consent.\\nCookie Settings\\nAccept All\\n', metadata={'source': 'C:\\\\Users\\\\Tairo Kageyama\\\\Documents\\\\GitHub\\\\Python-fo-Natural-Language-Processing-main\\\\lab7\\\\Data\\\\AIT_web_reduced.pdf', 'file_path': 'C:\\\\Users\\\\Tairo Kageyama\\\\Documents\\\\GitHub\\\\Python-fo-Natural-Language-Processing-main\\\\lab7\\\\Data\\\\AIT_web_reduced.pdf', 'page': 0, 'total_pages': 8, 'format': 'PDF 1.4', 'title': '', 'author': '', 'subject': '', 'keywords': '', 'creator': '', 'producer': 'iLovePDF', 'creationDate': '', 'modDate': 'D:20240319123827Z', 'trapped': ''})"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "documents[0]"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.2 Document Transformers\n",
    "\n",
    "This text splitter is the recommended one for generic text. It is parameterized by a list of characters. It tries to split on them in order until the chunks are small enough"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.text_splitter import RecursiveCharacterTextSplitter\n",
    "\n",
    "text_splitter = RecursiveCharacterTextSplitter(\n",
    "    chunk_size = 700,\n",
    "    chunk_overlap = 100\n",
    ")\n",
    "\n",
    "doc = text_splitter.split_documents(documents)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Document(page_content='intelligence.\\xa0 AI has two primary “levels”.\\xa0 The ﬁrst level, “narrow AI”, concerns perception, statistical\\ninference, and actuation, drawing on data science, sensors, and robotics.\\xa0 The second level, sometimes\\ncalled “artiﬁcial general intelligence (AGI), is concerned with more complex or ﬂexible reasoning and\\ndecision-making in less constrained domains.\\nThe AIT Masters in DS&AI was designed in partnership with the Erasmus+ DS&AI consortium, a group\\nof 15 European and Asian universities with the mission of bringing European-standard advanced\\neducation to Asia.\\nst\\nWe use cookies on our website to give you the most relevant experience by remembering your preferences and', metadata={'source': 'C:\\\\Users\\\\Tairo Kageyama\\\\Documents\\\\GitHub\\\\Python-fo-Natural-Language-Processing-main\\\\lab7\\\\Data\\\\AIT_web_reduced.pdf', 'file_path': 'C:\\\\Users\\\\Tairo Kageyama\\\\Documents\\\\GitHub\\\\Python-fo-Natural-Language-Processing-main\\\\lab7\\\\Data\\\\AIT_web_reduced.pdf', 'page': 0, 'total_pages': 8, 'format': 'PDF 1.4', 'title': '', 'author': '', 'subject': '', 'keywords': '', 'creator': '', 'producer': 'iLovePDF', 'creationDate': '', 'modDate': 'D:20240319123827Z', 'trapped': ''})"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "doc[1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "15"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(doc)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.3 Text Embedding Models\n",
    "Embeddings create a vector representation of a piece of text. This is useful because it means we can think about text in the vector space, and do things like semantic search where we look for pieces of text that are most similar in the vector space.\n",
    "\n",
    "*Note* Instructor Model : [Huggingface](gingface.co/hkunlp/instructor-base) | [Paper](https://arxiv.org/abs/2212.09741)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "load INSTRUCTOR_Transformer\n",
      "max_seq_length  512\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "from langchain.embeddings import HuggingFaceInstructEmbeddings\n",
    "\n",
    "model_name = 'hkunlp/instructor-base'\n",
    "\n",
    "embedding_model = HuggingFaceInstructEmbeddings(\n",
    "    model_name = model_name,\n",
    "    model_kwargs = {\"device\" : device}\n",
    ")"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.4 Vector Stores\n",
    "\n",
    "One of the most common ways to store and search over unstructured data is to embed it and store the resulting embedding vectors, and then at query time to embed the unstructured query and retrieve the embedding vectors that are 'most similar' to the embedded query. A vector store takes care of storing embedded data and performing vector search for you."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "#locate vectorstore\n",
    "vector_path = '../vector-store'\n",
    "if not os.path.exists(vector_path):\n",
    "    os.makedirs(vector_path)\n",
    "    print('create path done')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "#save vector locally\n",
    "from langchain.vectorstores import FAISS\n",
    "\n",
    "vectordb = FAISS.from_documents(\n",
    "    documents = doc,\n",
    "    embedding = embedding_model\n",
    ")\n",
    "\n",
    "db_file_name = 'nlp_stanford'\n",
    "\n",
    "vectordb.save_local(\n",
    "    folder_path = os.path.join(vector_path, db_file_name),\n",
    "    index_name = 'nlp' #default index\n",
    ")"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.5 retrievers\n",
    "A retriever is an interface that returns documents given an unstructured query. It is more general than a vector store. A retriever does not need to be able to store documents, only to return (or retrieve) them. Vector stores can be used as the backbone of a retriever, but there are other types of retrievers as well."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "#calling vector from local\n",
    "vector_path = '../vector-store'\n",
    "db_file_name = 'nlp_stanford'\n",
    "\n",
    "from langchain.vectorstores import FAISS\n",
    "\n",
    "vectordb = FAISS.load_local(\n",
    "    folder_path = os.path.join(vector_path, db_file_name),\n",
    "    embeddings = embedding_model,\n",
    "    index_name = 'nlp', #default index\n",
    "    # allow_dangerous_deserialization=True\n",
    ")   "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "#ready to use\n",
    "retriever = vectordb.as_retriever()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[Document(page_content='integration into the global economy. AIT focuses on assisting stakeholders build their capacity to\\npromote sustainability through appropriate technology, relevant and applied research, sustainable\\nframeworks for development and planning, informed policy making and practice applications in the\\nregion.\\xa0AIT’s ﬁve thematic areas of research are, namely, Climate Change; Smart Communities;\\nInfrastructure; Technology, Policy, Society and Water-Energy-Food.\\xa0\\nIn this sec�on\\nWe use cookies on our website to give you the most relevant experience by remembering your preferences and\\nrepeat visits. By clicking “Accept All”, you consent to the use of ALL the cookies. However, you may visit \"Cookie', metadata={'source': 'C:\\\\Users\\\\Tairo Kageyama\\\\Documents\\\\GitHub\\\\Python-fo-Natural-Language-Processing-main\\\\lab7\\\\Data\\\\AIT_web_reduced.pdf', 'file_path': 'C:\\\\Users\\\\Tairo Kageyama\\\\Documents\\\\GitHub\\\\Python-fo-Natural-Language-Processing-main\\\\lab7\\\\Data\\\\AIT_web_reduced.pdf', 'page': 7, 'total_pages': 8, 'format': 'PDF 1.4', 'title': '', 'author': '', 'subject': '', 'keywords': '', 'creator': '', 'producer': 'iLovePDF', 'creationDate': '', 'modDate': 'D:20240319123827Z', 'trapped': ''}),\n",
       " Document(page_content='Research Focus Area\\nPreferred background\\nTo study in the DS&AI ﬁeld, the students should fulﬁll one of the following backgrounds.\\nCourse Structure\\nData modeling and management\\nMachine learning\\nData mining\\nData science\\nSensors\\nRobotics\\nSoftware development\\nArtiﬁcial intelligence\\nBig Data and Deep Learning\\nComputer Science/Computer Engineering/ICT.\\nEngineering background with work experience, mathematical skills, and programming skills.\\nDiverse backgrounds such as business, ﬁnance, or other non-engineering ﬁelds. Candidates must\\ntake a foundation course in calculus, discrete mathematics, linear algebra, and basic computer\\nprogramming.\\nProgram Brochure\\nDownload', metadata={'source': 'C:\\\\Users\\\\Tairo Kageyama\\\\Documents\\\\GitHub\\\\Python-fo-Natural-Language-Processing-main\\\\lab7\\\\Data\\\\AIT_web_reduced.pdf', 'file_path': 'C:\\\\Users\\\\Tairo Kageyama\\\\Documents\\\\GitHub\\\\Python-fo-Natural-Language-Processing-main\\\\lab7\\\\Data\\\\AIT_web_reduced.pdf', 'page': 1, 'total_pages': 8, 'format': 'PDF 1.4', 'title': '', 'author': '', 'subject': '', 'keywords': '', 'creator': '', 'producer': 'iLovePDF', 'creationDate': '', 'modDate': 'D:20240319123827Z', 'trapped': ''}),\n",
       " Document(page_content='Apply Now\\nResearch Focus Area\\nPreferred background\\nCourse Structure\\nConcerned Faculty\\nStaff\\nContact Us\\nContact Us\\nComputer Science & Information Management (CSIM)\\nDepartment of ICT\\xa0\\nSchool of Engineering and Technology (SET)\\xa0\\nAsian Institute of Technology (AIT)\\xa0\\nPhone:+66(0)25245700; +66(0)25245717\\xa0\\nEmail:dsai@ait.ac.th\\nSiriporn Nanthasing\\nSireekant\\nThanwongpan', metadata={'source': 'C:\\\\Users\\\\Tairo Kageyama\\\\Documents\\\\GitHub\\\\Python-fo-Natural-Language-Processing-main\\\\lab7\\\\Data\\\\AIT_web_reduced.pdf', 'file_path': 'C:\\\\Users\\\\Tairo Kageyama\\\\Documents\\\\GitHub\\\\Python-fo-Natural-Language-Processing-main\\\\lab7\\\\Data\\\\AIT_web_reduced.pdf', 'page': 5, 'total_pages': 8, 'format': 'PDF 1.4', 'title': '', 'author': '', 'subject': '', 'keywords': '', 'creator': '', 'producer': 'iLovePDF', 'creationDate': '', 'modDate': 'D:20240319123827Z', 'trapped': ''}),\n",
       " Document(page_content='Home > Research Overview\\nResearch Overview\\nRecognized as one of the leading academic institutes in the ﬁeld of sustainability in Asia, AIT through its\\nresearch projects, thrives to take-on new research frontiers to build a better knowledge base and\\nfacilitate actions towards the sustainable development of the region. With an aim to make a signiﬁcant\\nand lasting difference to the communities, AIT’s research projects are designed to bolster technology\\ndevelopment and application, environmental conservation, policy innovation thus promoting\\nsustainability and sustainable development around Asia and beyond.', metadata={'source': 'C:\\\\Users\\\\Tairo Kageyama\\\\Documents\\\\GitHub\\\\Python-fo-Natural-Language-Processing-main\\\\lab7\\\\Data\\\\AIT_web_reduced.pdf', 'file_path': 'C:\\\\Users\\\\Tairo Kageyama\\\\Documents\\\\GitHub\\\\Python-fo-Natural-Language-Processing-main\\\\lab7\\\\Data\\\\AIT_web_reduced.pdf', 'page': 7, 'total_pages': 8, 'format': 'PDF 1.4', 'title': '', 'author': '', 'subject': '', 'keywords': '', 'creator': '', 'producer': 'iLovePDF', 'creationDate': '', 'modDate': 'D:20240319123827Z', 'trapped': ''})]"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "retriever.get_relevant_documents(\"What is main topic?\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[Document(page_content='Home > Data Science and AI (DSAI)\\nData Science and AI (DSAI)\\nData Science (DS) is concerned with the extraction of useful knowledge from data sets.\\xa0 It is closely\\nrelated to the ﬁelds of computer science, mathematics, and statistics.\\xa0 It is a relatively new term for a\\nbroad set of skills spanning the more established ﬁelds of machine learning, data mining, databases, and\\nvisualization, along with their applications in various ﬁelds.\\xa0 In 2012, Harvard Business Review called\\ndata science “The Sexiest Job of the 21\\n Century”.\\nAr�ﬁcial Intelligence (AI) is the broad ﬁeld conceived in 1956 as the automation or simulation of human', metadata={'source': 'C:\\\\Users\\\\Tairo Kageyama\\\\Documents\\\\GitHub\\\\Python-fo-Natural-Language-Processing-main\\\\lab7\\\\Data\\\\AIT_web_reduced.pdf', 'file_path': 'C:\\\\Users\\\\Tairo Kageyama\\\\Documents\\\\GitHub\\\\Python-fo-Natural-Language-Processing-main\\\\lab7\\\\Data\\\\AIT_web_reduced.pdf', 'page': 0, 'total_pages': 8, 'format': 'PDF 1.4', 'title': '', 'author': '', 'subject': '', 'keywords': '', 'creator': '', 'producer': 'iLovePDF', 'creationDate': '', 'modDate': 'D:20240319123827Z', 'trapped': ''}),\n",
       " Document(page_content='Research\\nStudy\\nOption\\nConcerned Faculty\\nStaff\\nChu�porn\\nAnutariya\\nAssociate Professor\\nChaklam\\nSilpasuwanchai\\nAssistant Professor\\nVatcharaporn\\nEsichaikul\\nAssociate Professor', metadata={'source': 'C:\\\\Users\\\\Tairo Kageyama\\\\Documents\\\\GitHub\\\\Python-fo-Natural-Language-Processing-main\\\\lab7\\\\Data\\\\AIT_web_reduced.pdf', 'file_path': 'C:\\\\Users\\\\Tairo Kageyama\\\\Documents\\\\GitHub\\\\Python-fo-Natural-Language-Processing-main\\\\lab7\\\\Data\\\\AIT_web_reduced.pdf', 'page': 4, 'total_pages': 8, 'format': 'PDF 1.4', 'title': '', 'author': '', 'subject': '', 'keywords': '', 'creator': '', 'producer': 'iLovePDF', 'creationDate': '', 'modDate': 'D:20240319123827Z', 'trapped': ''}),\n",
       " Document(page_content='Intelligence: Problem Solving and Planning · Ar�ﬁcial\\nIntelligence: Knowledge Representa�on and Reasoning\\n· Computer Vision · Ar�ﬁcial Intelligence: Natural\\nLanguage Understanding · Recent Trend in Machine\\nLearning · Mul�criteria Op�miza�on and Decision\\nAnalysis\\nTotal Credits\\nCoursework\\n36 credits\\nResearch Study\\nCredits\\n12 credits\\nTOTAL CREDIT\\nREQUIREMENT\\n48 credits', metadata={'source': 'C:\\\\Users\\\\Tairo Kageyama\\\\Documents\\\\GitHub\\\\Python-fo-Natural-Language-Processing-main\\\\lab7\\\\Data\\\\AIT_web_reduced.pdf', 'file_path': 'C:\\\\Users\\\\Tairo Kageyama\\\\Documents\\\\GitHub\\\\Python-fo-Natural-Language-Processing-main\\\\lab7\\\\Data\\\\AIT_web_reduced.pdf', 'page': 3, 'total_pages': 8, 'format': 'PDF 1.4', 'title': '', 'author': '', 'subject': '', 'keywords': '', 'creator': '', 'producer': 'iLovePDF', 'creationDate': '', 'modDate': 'D:20240319123827Z', 'trapped': ''}),\n",
       " Document(page_content='48 credit\\nData Modeling and Management\\nMachine Learning\\nBusiness Intelligence and Analytics\\nComputer Programming for Data Science and Artiﬁcial Intelligence\\nArtiﬁcial Intelligence: Natural Language Understanding\\nArtiﬁcial Intelligence: Knowledge Representation and Reasoning\\nComputer Vision\\nArtiﬁcial Intelligence: Problem Solving and Planning', metadata={'source': 'C:\\\\Users\\\\Tairo Kageyama\\\\Documents\\\\GitHub\\\\Python-fo-Natural-Language-Processing-main\\\\lab7\\\\Data\\\\AIT_web_reduced.pdf', 'file_path': 'C:\\\\Users\\\\Tairo Kageyama\\\\Documents\\\\GitHub\\\\Python-fo-Natural-Language-Processing-main\\\\lab7\\\\Data\\\\AIT_web_reduced.pdf', 'page': 2, 'total_pages': 8, 'format': 'PDF 1.4', 'title': '', 'author': '', 'subject': '', 'keywords': '', 'creator': '', 'producer': 'iLovePDF', 'creationDate': '', 'modDate': 'D:20240319123827Z', 'trapped': ''})]"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "retriever.get_relevant_documents(\"What is Claasfication?\")"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Memory\n",
    "\n",
    "One of the core utility classes underpinning most (if not all) memory modules is the ChatMessageHistory class. This is a super lightweight wrapper that provides convenience methods for saving HumanMessages, AIMessages, and then fetching them all.\n",
    "\n",
    "You may want to use this class directly if you are managing memory outside of a chain.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "# from langchain.memory import ChatMessageHistory\n",
    "\n",
    "# history = ChatMessageHistory()\n",
    "# history"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "# history.add_user_message('hi')\n",
    "# history.add_ai_message('Whats up?')\n",
    "# history.add_user_message('How are you')\n",
    "# history.add_ai_message('I\\'m quite good. How about you?')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "# history"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.1 Memory types\n",
    "\n",
    "There are many different types of memory. Each has their own parameters, their own return types, and is useful in different scenarios. \n",
    "- Converstaion Buffer\n",
    "- Converstaion Buffer Window"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "What variables get returned from memory\n",
    "\n",
    "Before going into the chain, various variables are read from memory. These have specific names which need to align with the variables the chain expects. You can see what these variables are by calling memory.load_memory_variables({}). Note that the empty dictionary that we pass in is just a placeholder for real variables. If the memory type you are using is dependent upon the input variables, you may need to pass some in."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this case, you can see that load_memory_variables returns a single key, history. This means that your chain (and likely your prompt) should expect an input named history. You can usually control this variable through parameters on the memory class. For example, if you want the memory variables to be returned in the key chat_history you can do:"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Converstaion Buffer\n",
    "This memory allows for storing messages and then extracts the messages in a variable."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'history': \"Human: hi\\nAI: What's up?\\nHuman: How are you?\\nAI: I'm quite good. How about you?\"}"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from langchain.memory import ConversationBufferMemory\n",
    "\n",
    "memory = ConversationBufferMemory()\n",
    "memory.save_context({'input':'hi'}, {'output':'What\\'s up?'})\n",
    "memory.save_context({\"input\":'How are you?'},{'output': 'I\\'m quite good. How about you?'})\n",
    "memory.load_memory_variables({})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'history': [HumanMessage(content='hi'),\n",
       "  AIMessage(content=\"What's up?\"),\n",
       "  HumanMessage(content='How are you?'),\n",
       "  AIMessage(content=\"I'm quite good. How about you?\")]}"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from langchain.memory import ConversationBufferMemory\n",
    "\n",
    "memory = ConversationBufferMemory(return_messages = True)\n",
    "memory.save_context({'input':'hi'}, {'output':'What\\'s up?'})\n",
    "memory.save_context({\"input\":'How are you?'},{'output': 'I\\'m quite good. How about you?'})\n",
    "memory.load_memory_variables({})"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Conversation Buffer Window\n",
    "- it keeps a list of the interactions of the conversation over time. \n",
    "- it only uses the last K interactions. \n",
    "- it can be useful for keeping a sliding window of the most recent interactions, so the buffer does not get too large."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'history': \"Human: How are you?\\nAI: I'm quite good. How about you?\"}"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from langchain.memory import ConversationBufferWindowMemory\n",
    "\n",
    "memory = ConversationBufferWindowMemory(k=1)\n",
    "memory.save_context({'input':'hi'}, {'output':'What\\'s up?'})\n",
    "memory.save_context({\"input\":'How are you?'},{'output': 'I\\'m quite good. How about you?'})\n",
    "memory.load_memory_variables({})"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. Chain\n",
    "\n",
    "Using an LLM in isolation is fine for simple applications, but more complex applications require chaining LLMs - either with each other or with other components.\n",
    "\n",
    "An `LLMChain` is a simple chain that adds some functionality around language models.\n",
    "- it consists of a `PromptTemplate` and a `LM` (either an LLM or chat model).\n",
    "- it formats the prompt template using the input key values provided (and also memory key values, if available), \n",
    "- it passes the formatted string to LLM and returns the LLM output.\n",
    "\n",
    "Note : [Download Fastchat Model Here](https://huggingface.co/lmsys/fastchat-t5-3b-v1.0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "c:\\Users\\Tairo Kageyama\\Documents\\GitHub\\Python-fo-Natural-Language-Processing-main\\lab7\\models\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "fatal: destination path 'fastchat-t5-3b-v1.0' already exists and is not an empty directory.\n"
     ]
    }
   ],
   "source": [
    "%cd ./models\n",
    "!git clone https://huggingface.co/lmsys/fastchat-t5-3b-v1.0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "You are using the default legacy behaviour of the <class 'transformers.models.t5.tokenization_t5.T5Tokenizer'>. This is expected, and simply means that the `legacy` (previous) behavior will be used so nothing changes for you. If you want to use the new behaviour, set `legacy=False`. This should only be set if you understand what it means, and thouroughly read the reason why this was added as explained in https://github.com/huggingface/transformers/pull/24565\n",
      "Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.\n",
      "Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1\n",
      "1\n",
      "2\n",
      "3\n",
      "4\n"
     ]
    }
   ],
   "source": [
    "from transformers import AutoTokenizer, pipeline, AutoModelForSeq2SeqLM\n",
    "from transformers import BitsAndBytesConfig\n",
    "from langchain import HuggingFacePipeline\n",
    "from langchain_community.llms.huggingface_endpoint import (\n",
    "    HuggingFaceEndpoint,\n",
    ")\n",
    "import torch\n",
    "\n",
    "model_id = '../models/fastchat-t5-3b-v1.0/'\n",
    "print(1)\n",
    "tokenizer = AutoTokenizer.from_pretrained(\n",
    "    model_id)\n",
    "\n",
    "tokenizer.pad_token_id = tokenizer.eos_token_id\n",
    "print(1)\n",
    "bitsandbyte_config = BitsAndBytesConfig(\n",
    "    load_in_4bit = True,\n",
    "    bnb_4bit_quant_type = \"nf4\",\n",
    "    bnb_4bit_compute_dtype = torch.float16,\n",
    "    bnb_4bit_use_double_quant = True\n",
    ")\n",
    "print(2)\n",
    "model = AutoModelForSeq2SeqLM.from_pretrained(\n",
    "    model_id,\n",
    "    # quantization_config = bitsandbyte_config, #caution Nvidia\n",
    "    # device_map = 'auto',\n",
    "    load_in_8bit = False\n",
    ")\n",
    "print(3)\n",
    "pipe = pipeline(\n",
    "    task=\"text2text-generation\",\n",
    "    model=model,\n",
    "    tokenizer=tokenizer,\n",
    "    max_new_tokens = 1024,\n",
    "    model_kwargs = {\n",
    "        \"temperature\" : 0,\n",
    "        \"repetition_penalty\": 1.5\n",
    "    }\n",
    ")\n",
    "print(4)\n",
    "# hf_model_id = \"facebook/bart-large-cnn\"\n",
    "# hf_endpoint_url = f\"https://api-inference.huggingface.co/models/{hf_model_id}\"\n",
    "# llm = HuggingFaceEndpoint(\n",
    "#     task=\"summarization\",\n",
    "#     endpoint_url=hf_endpoint_url,\n",
    "# )\n",
    "# llm = HuggingFacePipeline.from_model_id(\n",
    "#     model_id=model_id,\n",
    "#     task=\"text2text-generation\",\n",
    "#     model_kwargs={\"temperature\": 0, \"max_length\": 1000},\n",
    "# )\n",
    "\n",
    "llm = HuggingFacePipeline(pipeline = pipe)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### [Class ConversationalRetrievalChain](https://api.python.langchain.com/en/latest/_modules/langchain/chains/conversational_retrieval/base.html#ConversationalRetrievalChain)\n",
    "\n",
    "- `retriever` : Retriever to use to fetch documents.\n",
    "\n",
    "- `combine_docs_chain` : The chain used to combine any retrieved documents.\n",
    "\n",
    "- `question_generator`: The chain used to generate a new question for the sake of retrieval. This chain will take in the current question (with variable question) and any chat history (with variable chat_history) and will produce a new standalone question to be used later on.\n",
    "\n",
    "- `return_source_documents` : Return the retrieved source documents as part of the final result.\n",
    "\n",
    "- `get_chat_history` : An optional function to get a string of the chat history. If None is provided, will use a default.\n",
    "\n",
    "- `return_generated_question` : Return the generated question as part of the final result.\n",
    "\n",
    "- `response_if_no_docs_found` : If specified, the chain will return a fixed response if no docs are found for the question.\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "`question_generator`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.chains import LLMChain\n",
    "from langchain.chains.conversational_retrieval.prompts import CONDENSE_QUESTION_PROMPT\n",
    "from langchain.memory import ConversationBufferWindowMemory\n",
    "from langchain.chains.question_answering import load_qa_chain\n",
    "from langchain.chains import ConversationalRetrievalChain"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "PromptTemplate(input_variables=['chat_history', 'question'], template='Given the following conversation and a follow up question, rephrase the follow up question to be a standalone question, in its original language.\\n\\nChat History:\\n{chat_history}\\nFollow Up Input: {question}\\nStandalone question:')"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "CONDENSE_QUESTION_PROMPT"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "# hf_model_id = \"facebook/bart-large-cnn\"\n",
    "# hf_endpoint_url = f\"https://api-inference.huggingface.co/models/{hf_model_id}\"\n",
    "# llm = HuggingFaceEndpoint(\n",
    "#     task=\"summarization\",\n",
    "#     endpoint_url=hf_endpoint_url,\n",
    "# )\n",
    "# llm = HuggingFacePipeline(pipeline = pipe)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "question_generator = LLMChain(\n",
    "    llm = llm,\n",
    "    prompt = CONDENSE_QUESTION_PROMPT,\n",
    "    verbose = True\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "\u001b[1m> Entering new LLMChain chain...\u001b[0m\n",
      "Prompt after formatting:\n",
      "\u001b[32;1m\u001b[1;3mGiven the following conversation and a follow up question, rephrase the follow up question to be a standalone question, in its original language.\n",
      "\n",
      "Chat History:\n",
      "I am thinking to go to AIT\n",
      "Follow Up Input: What is AIT\n",
      "Standalone question:\u001b[0m\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\u001b[1m> Finished chain.\u001b[0m\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "{'chat_history': 'I am thinking to go to AIT',\n",
       " 'question': 'What is AIT',\n",
       " 'text': '<pad> What  is  AIT?\\n'}"
      ]
     },
     "execution_count": 33,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# from langchain.chains import LLMChain\n",
    "# from langchain.chains.conversational_retrieval.prompts import CONDENSE_QUESTION_PROMPT\n",
    "# from langchain.memory import ConversationBufferWindowMemory\n",
    "# from langchain.chains.question_answering import load_qa_chain\n",
    "# from langchain.chains import ConversationalRetrievalChain\n",
    "# llm = HuggingFacePipeline(pipeline = pipe)\n",
    "# question_generator = LLMChain(\n",
    "#     llm = llm,\n",
    "#     prompt = CONDENSE_QUESTION_PROMPT,\n",
    "#     verbose = True\n",
    "# )\n",
    "\n",
    "query = 'What is AIT'\n",
    "chat_history = \"I am thinking to go to AIT\"\n",
    "\n",
    "# question_generator({'context' : chat_history, \"question\" : query})\n",
    "question_generator({'chat_history' : chat_history, \"question\" : query})\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "`combine_docs_chain`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "HuggingFacePipeline(pipeline=<transformers.pipelines.text2text_generation.Text2TextGenerationPipeline object at 0x000001B69DD611B0>)"
      ]
     },
     "execution_count": 34,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "llm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "StuffDocumentsChain(verbose=True, llm_chain=LLMChain(verbose=True, prompt=PromptTemplate(input_variables=['context', 'question'], template=\"I'm your friendly chatbot named AIT-GPT, here to assist you with any questions you have about AIT. \\n    If you are interesting about Background of AIT, Programs, Degrees, Admissions, Tuitions fee, Ongoing Research, anything about AIT, please feel free to ask.\\n    I am covering everything in official web page. So if you felt lazy to search in web page by yourself, no hesitate to asking me.\\n    {context}\\n    Question: {question}\\n    Answer:\"), llm=HuggingFacePipeline(pipeline=<transformers.pipelines.text2text_generation.Text2TextGenerationPipeline object at 0x000001B69DD611B0>)), document_variable_name='context')"
      ]
     },
     "execution_count": 35,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "doc_chain = load_qa_chain(\n",
    "    llm = llm,\n",
    "    chain_type = 'stuff',\n",
    "    prompt = PROMPT,\n",
    "    verbose = True\n",
    ")\n",
    "doc_chain"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "\u001b[1m> Entering new StuffDocumentsChain chain...\u001b[0m"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "\n",
      "\u001b[1m> Entering new LLMChain chain...\u001b[0m\n",
      "Prompt after formatting:\n",
      "\u001b[32;1m\u001b[1;3mI'm your friendly chatbot named AIT-GPT, here to assist you with any questions you have about AIT. \n",
      "    If you are interesting about Background of AIT, Programs, Degrees, Admissions, Tuitions fee, Ongoing Research, anything about AIT, please feel free to ask.\n",
      "    I am covering everything in official web page. So if you felt lazy to search in web page by yourself, no hesitate to asking me.\n",
      "    integration into the global economy. AIT focuses on assisting stakeholders build their capacity to\n",
      "promote sustainability through appropriate technology, relevant and applied research, sustainable\n",
      "frameworks for development and planning, informed policy making and practice applications in the\n",
      "region. AIT’s ﬁve thematic areas of research are, namely, Climate Change; Smart Communities;\n",
      "Infrastructure; Technology, Policy, Society and Water-Energy-Food. \n",
      "In this sec�on\n",
      "We use cookies on our website to give you the most relevant experience by remembering your preferences and\n",
      "repeat visits. By clicking “Accept All”, you consent to the use of ALL the cookies. However, you may visit \"Cookie\n",
      "\n",
      "48 credit\n",
      "Data Modeling and Management\n",
      "Machine Learning\n",
      "Business Intelligence and Analytics\n",
      "Computer Programming for Data Science and Artiﬁcial Intelligence\n",
      "Artiﬁcial Intelligence: Natural Language Understanding\n",
      "Artiﬁcial Intelligence: Knowledge Representation and Reasoning\n",
      "Computer Vision\n",
      "Artiﬁcial Intelligence: Problem Solving and Planning\n",
      "\n",
      "Intelligence: Problem Solving and Planning · Ar�ﬁcial\n",
      "Intelligence: Knowledge Representa�on and Reasoning\n",
      "· Computer Vision · Ar�ﬁcial Intelligence: Natural\n",
      "Language Understanding · Recent Trend in Machine\n",
      "Learning · Mul�criteria Op�miza�on and Decision\n",
      "Analysis\n",
      "Total Credits\n",
      "Coursework\n",
      "36 credits\n",
      "Research Study\n",
      "Credits\n",
      "12 credits\n",
      "TOTAL CREDIT\n",
      "REQUIREMENT\n",
      "48 credits\n",
      "\n",
      "Home > Research Overview\n",
      "Research Overview\n",
      "Recognized as one of the leading academic institutes in the ﬁeld of sustainability in Asia, AIT through its\n",
      "research projects, thrives to take-on new research frontiers to build a better knowledge base and\n",
      "facilitate actions towards the sustainable development of the region. With an aim to make a signiﬁcant\n",
      "and lasting difference to the communities, AIT’s research projects are designed to bolster technology\n",
      "development and application, environmental conservation, policy innovation thus promoting\n",
      "sustainability and sustainable development around Asia and beyond.\n",
      "    Question: What is Transformers?\n",
      "    Answer:\u001b[0m\n",
      "\n",
      "\u001b[1m> Finished chain.\u001b[0m\n",
      "\n",
      "\u001b[1m> Finished chain.\u001b[0m\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "{'input_documents': [Document(page_content='integration into the global economy. AIT focuses on assisting stakeholders build their capacity to\\npromote sustainability through appropriate technology, relevant and applied research, sustainable\\nframeworks for development and planning, informed policy making and practice applications in the\\nregion.\\xa0AIT’s ﬁve thematic areas of research are, namely, Climate Change; Smart Communities;\\nInfrastructure; Technology, Policy, Society and Water-Energy-Food.\\xa0\\nIn this sec�on\\nWe use cookies on our website to give you the most relevant experience by remembering your preferences and\\nrepeat visits. By clicking “Accept All”, you consent to the use of ALL the cookies. However, you may visit \"Cookie', metadata={'source': 'C:\\\\Users\\\\Tairo Kageyama\\\\Documents\\\\GitHub\\\\Python-fo-Natural-Language-Processing-main\\\\lab7\\\\Data\\\\AIT_web_reduced.pdf', 'file_path': 'C:\\\\Users\\\\Tairo Kageyama\\\\Documents\\\\GitHub\\\\Python-fo-Natural-Language-Processing-main\\\\lab7\\\\Data\\\\AIT_web_reduced.pdf', 'page': 7, 'total_pages': 8, 'format': 'PDF 1.4', 'title': '', 'author': '', 'subject': '', 'keywords': '', 'creator': '', 'producer': 'iLovePDF', 'creationDate': '', 'modDate': 'D:20240319123827Z', 'trapped': ''}),\n",
       "  Document(page_content='48 credit\\nData Modeling and Management\\nMachine Learning\\nBusiness Intelligence and Analytics\\nComputer Programming for Data Science and Artiﬁcial Intelligence\\nArtiﬁcial Intelligence: Natural Language Understanding\\nArtiﬁcial Intelligence: Knowledge Representation and Reasoning\\nComputer Vision\\nArtiﬁcial Intelligence: Problem Solving and Planning', metadata={'source': 'C:\\\\Users\\\\Tairo Kageyama\\\\Documents\\\\GitHub\\\\Python-fo-Natural-Language-Processing-main\\\\lab7\\\\Data\\\\AIT_web_reduced.pdf', 'file_path': 'C:\\\\Users\\\\Tairo Kageyama\\\\Documents\\\\GitHub\\\\Python-fo-Natural-Language-Processing-main\\\\lab7\\\\Data\\\\AIT_web_reduced.pdf', 'page': 2, 'total_pages': 8, 'format': 'PDF 1.4', 'title': '', 'author': '', 'subject': '', 'keywords': '', 'creator': '', 'producer': 'iLovePDF', 'creationDate': '', 'modDate': 'D:20240319123827Z', 'trapped': ''}),\n",
       "  Document(page_content='Intelligence: Problem Solving and Planning · Ar�ﬁcial\\nIntelligence: Knowledge Representa�on and Reasoning\\n· Computer Vision · Ar�ﬁcial Intelligence: Natural\\nLanguage Understanding · Recent Trend in Machine\\nLearning · Mul�criteria Op�miza�on and Decision\\nAnalysis\\nTotal Credits\\nCoursework\\n36 credits\\nResearch Study\\nCredits\\n12 credits\\nTOTAL CREDIT\\nREQUIREMENT\\n48 credits', metadata={'source': 'C:\\\\Users\\\\Tairo Kageyama\\\\Documents\\\\GitHub\\\\Python-fo-Natural-Language-Processing-main\\\\lab7\\\\Data\\\\AIT_web_reduced.pdf', 'file_path': 'C:\\\\Users\\\\Tairo Kageyama\\\\Documents\\\\GitHub\\\\Python-fo-Natural-Language-Processing-main\\\\lab7\\\\Data\\\\AIT_web_reduced.pdf', 'page': 3, 'total_pages': 8, 'format': 'PDF 1.4', 'title': '', 'author': '', 'subject': '', 'keywords': '', 'creator': '', 'producer': 'iLovePDF', 'creationDate': '', 'modDate': 'D:20240319123827Z', 'trapped': ''}),\n",
       "  Document(page_content='Home > Research Overview\\nResearch Overview\\nRecognized as one of the leading academic institutes in the ﬁeld of sustainability in Asia, AIT through its\\nresearch projects, thrives to take-on new research frontiers to build a better knowledge base and\\nfacilitate actions towards the sustainable development of the region. With an aim to make a signiﬁcant\\nand lasting difference to the communities, AIT’s research projects are designed to bolster technology\\ndevelopment and application, environmental conservation, policy innovation thus promoting\\nsustainability and sustainable development around Asia and beyond.', metadata={'source': 'C:\\\\Users\\\\Tairo Kageyama\\\\Documents\\\\GitHub\\\\Python-fo-Natural-Language-Processing-main\\\\lab7\\\\Data\\\\AIT_web_reduced.pdf', 'file_path': 'C:\\\\Users\\\\Tairo Kageyama\\\\Documents\\\\GitHub\\\\Python-fo-Natural-Language-Processing-main\\\\lab7\\\\Data\\\\AIT_web_reduced.pdf', 'page': 7, 'total_pages': 8, 'format': 'PDF 1.4', 'title': '', 'author': '', 'subject': '', 'keywords': '', 'creator': '', 'producer': 'iLovePDF', 'creationDate': '', 'modDate': 'D:20240319123827Z', 'trapped': ''})],\n",
       " 'question': 'What is Transformers?',\n",
       " 'output_text': '<pad>  Transformers  is  a  term  used  to  describe  a  type  of  electrical  transformer  that  converts  one  form  of  electrical  energy  into  another.  Transformers  are  used  in  a  variety  of  applications,  including  power  generation,  distribution,  and  transmission.  They  are  typically  used  in  applications  where  the  electrical  energy  is  generated  by  a  power  source  and  needs  to  be  converted  to  another  form  before  it  can  be  used.  Transformers  are  typically  used  in  applications  where  the  electrical  energy  is  generated  by  a  power  source  that  is  not  suitable  for  direct  current  (DC)  power  sources,  such  as  wind  turbines  or  solar  panels.\\n'}"
      ]
     },
     "execution_count": 36,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "query = \"What is Transformers?\"\n",
    "input_document = retriever.get_relevant_documents(query)\n",
    "\n",
    "doc_chain({'input_documents':input_document, 'question':query})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "ConversationalRetrievalChain(memory=ConversationBufferWindowMemory(output_key='answer', return_messages=True, memory_key='chat_history', k=3), verbose=True, combine_docs_chain=StuffDocumentsChain(verbose=True, llm_chain=LLMChain(verbose=True, prompt=PromptTemplate(input_variables=['context', 'question'], template=\"I'm your friendly chatbot named AIT-GPT, here to assist you with any questions you have about AIT. \\n    If you are interesting about Background of AIT, Programs, Degrees, Admissions, Tuitions fee, Ongoing Research, anything about AIT, please feel free to ask.\\n    I am covering everything in official web page. So if you felt lazy to search in web page by yourself, no hesitate to asking me.\\n    {context}\\n    Question: {question}\\n    Answer:\"), llm=HuggingFacePipeline(pipeline=<transformers.pipelines.text2text_generation.Text2TextGenerationPipeline object at 0x000001B69DD611B0>)), document_variable_name='context'), question_generator=LLMChain(verbose=True, prompt=PromptTemplate(input_variables=['chat_history', 'question'], template='Given the following conversation and a follow up question, rephrase the follow up question to be a standalone question, in its original language.\\n\\nChat History:\\n{chat_history}\\nFollow Up Input: {question}\\nStandalone question:'), llm=HuggingFacePipeline(pipeline=<transformers.pipelines.text2text_generation.Text2TextGenerationPipeline object at 0x000001B69DD611B0>)), return_source_documents=True, get_chat_history=<function <lambda> at 0x000001B73E133910>, retriever=VectorStoreRetriever(tags=['FAISS', 'HuggingFaceInstructEmbeddings'], vectorstore=<langchain_community.vectorstores.faiss.FAISS object at 0x000001B6DC327940>))"
      ]
     },
     "execution_count": 37,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "memory = ConversationBufferWindowMemory(\n",
    "    k=3, \n",
    "    memory_key = \"chat_history\",\n",
    "    return_messages = True,\n",
    "    output_key = 'answer'\n",
    ")\n",
    "\n",
    "chain = ConversationalRetrievalChain(\n",
    "    retriever=retriever,\n",
    "    question_generator=question_generator,\n",
    "    combine_docs_chain=doc_chain,\n",
    "    return_source_documents=True,\n",
    "    memory=memory,\n",
    "    verbose=True,\n",
    "    get_chat_history=lambda h : h\n",
    ")\n",
    "chain"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "ename": "PicklingError",
     "evalue": "Can't pickle <function <lambda> at 0x000001B73E133910>: attribute lookup <lambda> on __main__ failed",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mPicklingError\u001b[0m                             Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[38], line 3\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mpickle\u001b[39;00m\n\u001b[0;32m      2\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m \u001b[38;5;28mopen\u001b[39m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mchain.pkl\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mwb\u001b[39m\u001b[38;5;124m\"\u001b[39m) \u001b[38;5;28;01mas\u001b[39;00m f:\n\u001b[1;32m----> 3\u001b[0m     \u001b[43mpickle\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mdump\u001b[49m\u001b[43m(\u001b[49m\u001b[43mchain\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mf\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[1;31mPicklingError\u001b[0m: Can't pickle <function <lambda> at 0x000001B73E133910>: attribute lookup <lambda> on __main__ failed"
     ]
    }
   ],
   "source": [
    "# import pickle\n",
    "# with open(\"chain.pkl\", \"wb\") as f:\n",
    "#     pickle.dump(chain, f)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5. Chatbot"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "\u001b[1m> Entering new ConversationalRetrievalChain chain...\u001b[0m\n",
      "\n",
      "\n",
      "\u001b[1m> Entering new StuffDocumentsChain chain...\u001b[0m\n",
      "\n",
      "\n",
      "\u001b[1m> Entering new LLMChain chain...\u001b[0m\n",
      "Prompt after formatting:\n",
      "\u001b[32;1m\u001b[1;3mI'm your friendly chatbot named AIT-GPT, here to assist you with any questions you have about AIT. \n",
      "    If you are interesting about Background of AIT, Programs, Degrees, Admissions, Tuitions fee, Ongoing Research, anything about AIT, please feel free to ask.\n",
      "    I am covering everything in official web page. So if you felt lazy to search in web page by yourself, no hesitate to asking me.\n",
      "    Intelligence: Problem Solving and Planning · Ar�ﬁcial\n",
      "Intelligence: Knowledge Representa�on and Reasoning\n",
      "· Computer Vision · Ar�ﬁcial Intelligence: Natural\n",
      "Language Understanding · Recent Trend in Machine\n",
      "Learning · Mul�criteria Op�miza�on and Decision\n",
      "Analysis\n",
      "Total Credits\n",
      "Coursework\n",
      "36 credits\n",
      "Research Study\n",
      "Credits\n",
      "12 credits\n",
      "TOTAL CREDIT\n",
      "REQUIREMENT\n",
      "48 credits\n",
      "\n",
      "intelligence.  AI has two primary “levels”.  The ﬁrst level, “narrow AI”, concerns perception, statistical\n",
      "inference, and actuation, drawing on data science, sensors, and robotics.  The second level, sometimes\n",
      "called “artiﬁcial general intelligence (AGI), is concerned with more complex or ﬂexible reasoning and\n",
      "decision-making in less constrained domains.\n",
      "The AIT Masters in DS&AI was designed in partnership with the Erasmus+ DS&AI consortium, a group\n",
      "of 15 European and Asian universities with the mission of bringing European-standard advanced\n",
      "education to Asia.\n",
      "st\n",
      "We use cookies on our website to give you the most relevant experience by remembering your preferences and\n",
      "\n",
      "Research\n",
      "Study\n",
      "Option\n",
      "Concerned Faculty\n",
      "Staff\n",
      "Chu�porn\n",
      "Anutariya\n",
      "Associate Professor\n",
      "Chaklam\n",
      "Silpasuwanchai\n",
      "Assistant Professor\n",
      "Vatcharaporn\n",
      "Esichaikul\n",
      "Associate Professor\n",
      "\n",
      "48 credit\n",
      "Data Modeling and Management\n",
      "Machine Learning\n",
      "Business Intelligence and Analytics\n",
      "Computer Programming for Data Science and Artiﬁcial Intelligence\n",
      "Artiﬁcial Intelligence: Natural Language Understanding\n",
      "Artiﬁcial Intelligence: Knowledge Representation and Reasoning\n",
      "Computer Vision\n",
      "Artiﬁcial Intelligence: Problem Solving and Planning\n",
      "    Question: Who are you by the way?\n",
      "    Answer:\u001b[0m\n",
      "\n",
      "\u001b[1m> Finished chain.\u001b[0m\n",
      "\n",
      "\u001b[1m> Finished chain.\u001b[0m\n",
      "\n",
      "\u001b[1m> Finished chain.\u001b[0m\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "{'question': 'Who are you by the way?',\n",
       " 'chat_history': [],\n",
       " 'answer': \"<pad>  I  am  AIT-GPT,  a  chatbot  designed  to  assist  you  with  information  about  AIT.  I  am  a  language  model  trained  on  AIT's  website  and  can  provide  information  about  AIT's  programs,  degrees,  admissions,  tuitions  fee,  ongoing  research,  anything  about  AIT.  I  am  not  a  human  being  and  I  am  not  able  to  make  decisions  or  take  actions  on  my  own.  I  am  here  to  assist  you  with  any  questions  you  have  about  AIT.\\n\",\n",
       " 'source_documents': [Document(page_content='Intelligence: Problem Solving and Planning · Ar�ﬁcial\\nIntelligence: Knowledge Representa�on and Reasoning\\n· Computer Vision · Ar�ﬁcial Intelligence: Natural\\nLanguage Understanding · Recent Trend in Machine\\nLearning · Mul�criteria Op�miza�on and Decision\\nAnalysis\\nTotal Credits\\nCoursework\\n36 credits\\nResearch Study\\nCredits\\n12 credits\\nTOTAL CREDIT\\nREQUIREMENT\\n48 credits', metadata={'source': 'C:\\\\Users\\\\Tairo Kageyama\\\\Documents\\\\GitHub\\\\Python-fo-Natural-Language-Processing-main\\\\lab7\\\\Data\\\\AIT_web_reduced.pdf', 'file_path': 'C:\\\\Users\\\\Tairo Kageyama\\\\Documents\\\\GitHub\\\\Python-fo-Natural-Language-Processing-main\\\\lab7\\\\Data\\\\AIT_web_reduced.pdf', 'page': 3, 'total_pages': 8, 'format': 'PDF 1.4', 'title': '', 'author': '', 'subject': '', 'keywords': '', 'creator': '', 'producer': 'iLovePDF', 'creationDate': '', 'modDate': 'D:20240319123827Z', 'trapped': ''}),\n",
       "  Document(page_content='intelligence.\\xa0 AI has two primary “levels”.\\xa0 The ﬁrst level, “narrow AI”, concerns perception, statistical\\ninference, and actuation, drawing on data science, sensors, and robotics.\\xa0 The second level, sometimes\\ncalled “artiﬁcial general intelligence (AGI), is concerned with more complex or ﬂexible reasoning and\\ndecision-making in less constrained domains.\\nThe AIT Masters in DS&AI was designed in partnership with the Erasmus+ DS&AI consortium, a group\\nof 15 European and Asian universities with the mission of bringing European-standard advanced\\neducation to Asia.\\nst\\nWe use cookies on our website to give you the most relevant experience by remembering your preferences and', metadata={'source': 'C:\\\\Users\\\\Tairo Kageyama\\\\Documents\\\\GitHub\\\\Python-fo-Natural-Language-Processing-main\\\\lab7\\\\Data\\\\AIT_web_reduced.pdf', 'file_path': 'C:\\\\Users\\\\Tairo Kageyama\\\\Documents\\\\GitHub\\\\Python-fo-Natural-Language-Processing-main\\\\lab7\\\\Data\\\\AIT_web_reduced.pdf', 'page': 0, 'total_pages': 8, 'format': 'PDF 1.4', 'title': '', 'author': '', 'subject': '', 'keywords': '', 'creator': '', 'producer': 'iLovePDF', 'creationDate': '', 'modDate': 'D:20240319123827Z', 'trapped': ''}),\n",
       "  Document(page_content='Research\\nStudy\\nOption\\nConcerned Faculty\\nStaff\\nChu�porn\\nAnutariya\\nAssociate Professor\\nChaklam\\nSilpasuwanchai\\nAssistant Professor\\nVatcharaporn\\nEsichaikul\\nAssociate Professor', metadata={'source': 'C:\\\\Users\\\\Tairo Kageyama\\\\Documents\\\\GitHub\\\\Python-fo-Natural-Language-Processing-main\\\\lab7\\\\Data\\\\AIT_web_reduced.pdf', 'file_path': 'C:\\\\Users\\\\Tairo Kageyama\\\\Documents\\\\GitHub\\\\Python-fo-Natural-Language-Processing-main\\\\lab7\\\\Data\\\\AIT_web_reduced.pdf', 'page': 4, 'total_pages': 8, 'format': 'PDF 1.4', 'title': '', 'author': '', 'subject': '', 'keywords': '', 'creator': '', 'producer': 'iLovePDF', 'creationDate': '', 'modDate': 'D:20240319123827Z', 'trapped': ''}),\n",
       "  Document(page_content='48 credit\\nData Modeling and Management\\nMachine Learning\\nBusiness Intelligence and Analytics\\nComputer Programming for Data Science and Artiﬁcial Intelligence\\nArtiﬁcial Intelligence: Natural Language Understanding\\nArtiﬁcial Intelligence: Knowledge Representation and Reasoning\\nComputer Vision\\nArtiﬁcial Intelligence: Problem Solving and Planning', metadata={'source': 'C:\\\\Users\\\\Tairo Kageyama\\\\Documents\\\\GitHub\\\\Python-fo-Natural-Language-Processing-main\\\\lab7\\\\Data\\\\AIT_web_reduced.pdf', 'file_path': 'C:\\\\Users\\\\Tairo Kageyama\\\\Documents\\\\GitHub\\\\Python-fo-Natural-Language-Processing-main\\\\lab7\\\\Data\\\\AIT_web_reduced.pdf', 'page': 2, 'total_pages': 8, 'format': 'PDF 1.4', 'title': '', 'author': '', 'subject': '', 'keywords': '', 'creator': '', 'producer': 'iLovePDF', 'creationDate': '', 'modDate': 'D:20240319123827Z', 'trapped': ''})]}"
      ]
     },
     "execution_count": 37,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "prompt_question = \"Who are you by the way?\"\n",
    "answer = chain({\"question\":prompt_question})\n",
    "answer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "\u001b[1m> Entering new ConversationalRetrievalChain chain...\u001b[0m\n",
      "\n",
      "\n",
      "\u001b[1m> Entering new StuffDocumentsChain chain...\u001b[0m\n",
      "\n",
      "\n",
      "\u001b[1m> Entering new LLMChain chain...\u001b[0m\n",
      "Prompt after formatting:\n",
      "\u001b[32;1m\u001b[1;3mI'm your friendly chatbot named AIT-GPT, here to assist you with any questions you have about AIT. \n",
      "    If you are interesting about Background of AIT, Programs, Degrees, Admissions, Tuitions fee, Ongoing Research, anything about AIT, please feel free to ask.\n",
      "    I am covering everything in official web page. So if you felt lazy to search in web page by yourself, no hesitate to asking me.\n",
      "    Home > Data Science and AI (DSAI)\n",
      "Data Science and AI (DSAI)\n",
      "Data Science (DS) is concerned with the extraction of useful knowledge from data sets.  It is closely\n",
      "related to the ﬁelds of computer science, mathematics, and statistics.  It is a relatively new term for a\n",
      "broad set of skills spanning the more established ﬁelds of machine learning, data mining, databases, and\n",
      "visualization, along with their applications in various ﬁelds.  In 2012, Harvard Business Review called\n",
      "data science “The Sexiest Job of the 21\n",
      " Century”.\n",
      "Ar�ﬁcial Intelligence (AI) is the broad ﬁeld conceived in 1956 as the automation or simulation of human\n",
      "\n",
      "intelligence.  AI has two primary “levels”.  The ﬁrst level, “narrow AI”, concerns perception, statistical\n",
      "inference, and actuation, drawing on data science, sensors, and robotics.  The second level, sometimes\n",
      "called “artiﬁcial general intelligence (AGI), is concerned with more complex or ﬂexible reasoning and\n",
      "decision-making in less constrained domains.\n",
      "The AIT Masters in DS&AI was designed in partnership with the Erasmus+ DS&AI consortium, a group\n",
      "of 15 European and Asian universities with the mission of bringing European-standard advanced\n",
      "education to Asia.\n",
      "st\n",
      "We use cookies on our website to give you the most relevant experience by remembering your preferences and\n",
      "\n",
      "Research Focus Area\n",
      "Preferred background\n",
      "To study in the DS&AI ﬁeld, the students should fulﬁll one of the following backgrounds.\n",
      "Course Structure\n",
      "Data modeling and management\n",
      "Machine learning\n",
      "Data mining\n",
      "Data science\n",
      "Sensors\n",
      "Robotics\n",
      "Software development\n",
      "Artiﬁcial intelligence\n",
      "Big Data and Deep Learning\n",
      "Computer Science/Computer Engineering/ICT.\n",
      "Engineering background with work experience, mathematical skills, and programming skills.\n",
      "Diverse backgrounds such as business, ﬁnance, or other non-engineering ﬁelds. Candidates must\n",
      "take a foundation course in calculus, discrete mathematics, linear algebra, and basic computer\n",
      "programming.\n",
      "Program Brochure\n",
      "Download\n",
      "\n",
      "Major in DS&AI\n",
      "Required courses\n",
      "Elective courses\n",
      "Master of Science/Master of Engineering in Data Science and Ar�ﬁcial Intelligence.\n",
      "Thesis Option\n",
      "Research Study Option\n",
      "Required Courses\n",
      "14 credits (5 courses)\n",
      "14 credits (5 courses)\n",
      "Elec�ve Courses\n",
      "6 credits (2 courses)\n",
      "18 credits (6 courses)\n",
      "Ins�tute Wide Courses\n",
      "3 credits (1 course)\n",
      "3 credits (1 course)\n",
      "Seminar: Required Pass/Fail\n",
      "1 credit\n",
      "1 credit\n",
      "Required Internship\n",
      "0 credit\n",
      "0 credit\n",
      "Total Credits Coursework\n",
      "24 credit\n",
      "36 credit\n",
      "Thesis/Research Study Credits\n",
      "24 credit\n",
      "12 credit\n",
      "TOTAL CREDIT REQUIREMENT\n",
      "48 credit\n",
      "48 credit\n",
      "Data Modeling and Management\n",
      "Machine Learning\n",
      "Business Intelligence and Analytics\n",
      "    Question: What is DSAI?\n",
      "    Answer:\u001b[0m\n",
      "\n",
      "\u001b[1m> Finished chain.\u001b[0m\n",
      "\n",
      "\u001b[1m> Finished chain.\u001b[0m\n",
      "\n",
      "\u001b[1m> Finished chain.\u001b[0m\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "'<pad>  Data  Science  and  Artificial  Intelligence  (DSAI)  is  a  broad  term  for  a  set  of  skills  spanning  the  more  established  fields  of  machine  learning,  data  mining,  databases,  and  visualization,  along  with  their  applications  in  various  fields.  It  is  concerned  with  the  extraction  of  useful  knowledge  from  data  sets  and  the  application  of  these  skills  in  various  fields.\\n'"
      ]
     },
     "execution_count": 39,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "prompt_question = \"What is DSAI?\"\n",
    "context = \"I want to go to AIT\"\n",
    "answer = chain({\"question\" : prompt_question})\n",
    "answer['answer']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# prompt_question = \"What is Eligibility?\"\n",
    "# answer = chain({'context' : chat_history, \"question\":prompt_question})\n",
    "# answer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Tairo Kageyama\\AppData\\Local\\Temp\\ipykernel_22680\\3703769464.py:2: UserWarning: \n",
      "The dash_core_components package is deprecated. Please replace\n",
      "`import dash_core_components as dcc` with `from dash import dcc`\n",
      "  import dash_core_components as dcc\n",
      "C:\\Users\\Tairo Kageyama\\AppData\\Local\\Temp\\ipykernel_22680\\3703769464.py:3: UserWarning: \n",
      "The dash_html_components package is deprecated. Please replace\n",
      "`import dash_html_components as html` with `from dash import html`\n",
      "  import dash_html_components as html\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "        <iframe\n",
       "            width=\"100%\"\n",
       "            height=\"650\"\n",
       "            src=\"http://127.0.0.1:8050/\"\n",
       "            frameborder=\"0\"\n",
       "            allowfullscreen\n",
       "            \n",
       "        ></iframe>\n",
       "        "
      ],
      "text/plain": [
       "<IPython.lib.display.IFrame at 0x1b6aaeaee60>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Top 1: Similarity=0.2626, Text= It is among the most notable theorems in the history of mathematics and prior to its proof was in the Guinness Book of World Records as the \"most difficult mathematical problem\"  in part because the theorem has the largest number of unsuccessful proofs\n",
      "\n",
      "Top 2: Similarity=0.2611, Text= The unsolved problem stimulated the development of algebraic number theory in the 19th and 20th centuries\n",
      "\n",
      "Top 3: Similarity=0.2500, Text= The proposition was first stated as a theorem by Pierre de Fermat around 1637 in the margin of a copy of Arithmetica\n",
      "\n",
      "Top 4: Similarity=0.2357, Text= After 358 years of effort by mathematicians  the first successful proof was released in 1994 by Andrew Wiles and formally published in 1995\n",
      "\n",
      "Top 5: Similarity=0.1342, Text= It also proved much of the Taniyama–Shimura conjecture  subsequently known as the modularity theorem  and opened up entire new approaches to numerous other problems and mathematically powerful modularity lifting techniques\n",
      "\n",
      "Top 6: Similarity=0.1291, Text= Consequently  the proposition became known as a conjecture rather than a theorem\n",
      "\n",
      "Top 7: Similarity=0.1054, Text= The cases n = 1 and n = 2 have been known since antiquity to have infinitely many solutions\n",
      "\n",
      "Top 8: Similarity=0.0990, Text= Fermat added that he had a proof that was too large to fit in the margin\n",
      "\n",
      "Top 9: Similarity=0.0962, Text= It was described as a \"stunning advance\" in the citation for Wiles's Abel Prize award in 2016\n",
      "\n",
      "Top 10: Similarity=0.0949, Text= Although other statements claimed by Fermat without proof were subsequently proven by others and credited as theorems of Fermat  for example  Fermat's theorem on sums of two squares   Fermat's Last Theorem resisted proof  leading to doubt that Fermat ever had a correct proof\n",
      "\n"
     ]
    }
   ],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
